{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "import pandas as pd\n",
    "import os #Import os package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Read in file\n",
    "#%%\n",
    "os.chdir('/home/yingjie/Desktop')\n",
    "os.chdir('/Users/eihoman/Documents/GitHub/Kaggle_Competition_4-8/')\n",
    "\n",
    "test_ID = pd.DataFrame.from_csv('test_ID.csv',index_col=None)\n",
    "test = pd.DataFrame.from_csv('testing.csv',index_col=None)\n",
    "train = pd.DataFrame.from_csv('training_weighted.csv',index_col=None)\n",
    "valid = pd.DataFrame.from_csv('validation_weighted.csv',index_col=None)\n",
    "targets = pd.DataFrame.from_csv('validation_target.csv',index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Make sure testing set was read in correctly\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Make sure training set was read in correctly\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Check zeros in training dataset\n",
    "sum(train.target==0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Did not use this part of the code (although we attempted to run gini in python prior to changing tactics)\n",
    "# Define unnormalized gini index and normalized gini index\n",
    "def unnormalized_gini_index(ground_truth, predicted_probabilities):\n",
    "    if (len(ground_truth) !=  len(predicted_probabilities)):\n",
    "        stop(\"Actual and Predicted need to be equal lengths!\")\n",
    "    x = len(ground_truth)   \n",
    "    gini_table = pd.DataFrame(index = list(range(1,x+1,1)), predicted_probabilities=predicted_probabilities, ground_truth=ground_truth)\n",
    "    gini_table = gini.table[order(-gini_table.predicted_probabilities, gini_table.index), ]\n",
    "    num_ground_truth_positivies = sum(gini_table.ground_truth)\n",
    "    model_percentage_positives_accumulated = gini_table.ground_truth / num_ground_truth_positivies\n",
    "    random_guess_percentage_positives_accumulated = 1 / nrow(gini_table)\n",
    "    gini_sum = cumsum(model_percentage_positives_accumulated - random_guess_percentage_positives_accumulated)\n",
    "    gini_index = sum(gini_sum) / nrow(gini_table) \n",
    "    return(gini_index)\n",
    "\n",
    "def normalized_gini_index(ground_truth, predicted_probabilities):\n",
    "    model_gini_index = unnormalized_gini_index(ground_truth, predicted_probabilities)\n",
    "    optimal_gini_index = unnormalized_gini_index(ground_truth, ground_truth)\n",
    "    return(model_gini_index / optimal_gini_index)\n",
    "\n",
    "#%%\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(train) #86776\n",
    "len(valid) #21694"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PRE-PROCESSING - SUBSET MODEL\n",
    "# Training set preparation\n",
    "train_RF_noT = train.loc[:,train_RF.columns !=\"target\"] # training set predictors\n",
    "train_RF_T = train[[\"target\"]]\n",
    "\n",
    "# Validation set preparation (for consistency)\n",
    "valid_RF_noT = valid\n",
    "valid_RF_T = targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Random Forest - Takes a very long time to run\n",
    "# IMPORTANT NOTE: we altered max_depth and max_features manually and tested with gini in R!\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=10, max_features=20, n_estimators=1000, random_state=0)\n",
    "clf.fit(train_RF_noT,train_RF_T) # Fit training dataset\n",
    "preds = (clf.predict(valid_RF_noT)) # Predict using validation set\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#For the max_depth=10, max_features=20, n_est=1000...\n",
    "preds10 = pd.DataFrame(preds)\n",
    "preds10.columns = [\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds10.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum(preds10.target==0)\n",
    "len(preds10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds10.to_csv(\"preds10.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set parameters to whatever worked best (these produces our highest kaggle score...)\n",
    "#NOTE: again, this was a semi-iterative process, as we made several kaggle submission for RF\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=100, max_features=30, n_estimators=1000, random_state=0)\n",
    "clf.fit(train_RF_noT,train_RF_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#See feature importances\n",
    "print(clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Predict on testing set\n",
    "preds = (clf.predict(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Place predictions into dataframe and label with \"target\"\n",
    "predsfinal = pd.DataFrame(preds)\n",
    "predsfinal.columns = [\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Check/view dataframe\n",
    "predsfinal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum(predsfinal.target==0) # Check how many predicted values are zeros\n",
    "len(predsfinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Write out to csv\n",
    "predsfinal.to_csv(\"finalpreds.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
